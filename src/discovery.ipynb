{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "* This notebook was used to review input data, design, build and test components for use in the main tool.\n",
    "\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import safe_load_json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from claude import call_claude\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "from base import BASE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client = chromadb.PersistentClient(path=f\"{BASE_DIR}/embeddings/voiyage-2\")\n",
    "\n",
    "from embeddings import call_vo_embeddings, count_tokens, chunk_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>document_title</th>\n",
       "      <td>CerenceInc_20191002_8-K_EX-10.4_11827494_EX-10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>Highlight the parts (if any) of this contract ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context</th>\n",
       "      <td>/2/2019\\n\\n\\n\\n\\n\\nparty agrees that such part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context_embedding</th>\n",
       "      <td>[[-0.01682035  0.04582816  0.06161021 ...  0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <td>specific performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gt_contexts</th>\n",
       "      <td>[exhibit 10.4\\n\\nintellectual property agreeme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gt_context_embedding</th>\n",
       "      <td>[-0.019412100315093994, 0.014370781369507313, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gt_answer_starts</th>\n",
       "      <td>[3012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gt_answers</th>\n",
       "      <td>[INTELLECTUAL PROPERTY AGREEMENT, d]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gt_is_impossible</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_taken</th>\n",
       "      <td>1.052552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collection_name</th>\n",
       "      <td>legal_docs_voiyage2_1024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      0\n",
       "document_title        CerenceInc_20191002_8-K_EX-10.4_11827494_EX-10...\n",
       "question              Highlight the parts (if any) of this contract ...\n",
       "context               /2/2019\\n\\n\\n\\n\\n\\nparty agrees that such part...\n",
       "context_embedding     [[-0.01682035  0.04582816  0.06161021 ...  0.0...\n",
       "answer                                             specific performance\n",
       "gt_contexts           [exhibit 10.4\\n\\nintellectual property agreeme...\n",
       "gt_context_embedding  [-0.019412100315093994, 0.014370781369507313, ...\n",
       "gt_answer_starts                                                 [3012]\n",
       "gt_answers                         [INTELLECTUAL PROPERTY AGREEMENT, d]\n",
       "gt_is_impossible                                                  False\n",
       "time_taken                                                     1.052552\n",
       "collection_name                                legal_docs_voiyage2_1024"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_main = pd.read_parquet(f\"{BASE_DIR}/output/main/method-002/data.parquet\")\n",
    "print(test_main.shape)\n",
    "test_main.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>collection_name</th>\n",
       "      <td>legal_docs_voiyage2_1024</td>\n",
       "      <td>legal_docs_voiyage2_2048</td>\n",
       "      <td>legal_docs_voiyage2_512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Reciprocal Rank (MRR)</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision at 5 (P@5)</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm_gt_containment_proportion</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm_unigram_match</th>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm_bigram_match</th>\n",
       "      <td>0.286458</td>\n",
       "      <td>0.473958</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm_trigram_match</th>\n",
       "      <td>0.09375</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.145833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm_precision</th>\n",
       "      <td>0.306744</td>\n",
       "      <td>0.382743</td>\n",
       "      <td>0.258761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm_recall</th>\n",
       "      <td>0.153882</td>\n",
       "      <td>0.255137</td>\n",
       "      <td>0.162771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm_f1_score</th>\n",
       "      <td>0.145472</td>\n",
       "      <td>0.255232</td>\n",
       "      <td>0.182372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm_r1_recall</th>\n",
       "      <td>0.150576</td>\n",
       "      <td>0.244837</td>\n",
       "      <td>0.161607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm_r1_precision</th>\n",
       "      <td>0.090278</td>\n",
       "      <td>0.177188</td>\n",
       "      <td>0.121734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm_r1_f1</th>\n",
       "      <td>0.088094</td>\n",
       "      <td>0.179694</td>\n",
       "      <td>0.129176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embeddings_bleu_score</th>\n",
       "      <td>0.032378</td>\n",
       "      <td>0.153684</td>\n",
       "      <td>0.025958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embeddings_r2_recall</th>\n",
       "      <td>0.608405</td>\n",
       "      <td>0.551241</td>\n",
       "      <td>0.599061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embeddings_r2_precision</th>\n",
       "      <td>0.121176</td>\n",
       "      <td>0.24712</td>\n",
       "      <td>0.087567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embeddings_r2_f1</th>\n",
       "      <td>0.196321</td>\n",
       "      <td>0.321197</td>\n",
       "      <td>0.143358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_taken</th>\n",
       "      <td>1.166716</td>\n",
       "      <td>1.299471</td>\n",
       "      <td>1.129063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impossible_score</th>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.327586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  \\\n",
       "collection_name                legal_docs_voiyage2_1024   \n",
       "Mean Reciprocal Rank (MRR)                     0.083333   \n",
       "Precision at 5 (P@5)                           0.083333   \n",
       "llm_gt_containment_proportion                       0.0   \n",
       "llm_unigram_match                              0.458333   \n",
       "llm_bigram_match                               0.286458   \n",
       "llm_trigram_match                               0.09375   \n",
       "llm_precision                                  0.306744   \n",
       "llm_recall                                     0.153882   \n",
       "llm_f1_score                                   0.145472   \n",
       "llm_r1_recall                                  0.150576   \n",
       "llm_r1_precision                               0.090278   \n",
       "llm_r1_f1                                      0.088094   \n",
       "embeddings_bleu_score                          0.032378   \n",
       "embeddings_r2_recall                           0.608405   \n",
       "embeddings_r2_precision                        0.121176   \n",
       "embeddings_r2_f1                               0.196321   \n",
       "time_taken                                     1.166716   \n",
       "impossible_score                               0.482759   \n",
       "\n",
       "                                                      1  \\\n",
       "collection_name                legal_docs_voiyage2_2048   \n",
       "Mean Reciprocal Rank (MRR)                     0.458333   \n",
       "Precision at 5 (P@5)                           0.458333   \n",
       "llm_gt_containment_proportion                  0.041667   \n",
       "llm_unigram_match                                0.6875   \n",
       "llm_bigram_match                               0.473958   \n",
       "llm_trigram_match                              0.145833   \n",
       "llm_precision                                  0.382743   \n",
       "llm_recall                                     0.255137   \n",
       "llm_f1_score                                   0.255232   \n",
       "llm_r1_recall                                  0.244837   \n",
       "llm_r1_precision                               0.177188   \n",
       "llm_r1_f1                                      0.179694   \n",
       "embeddings_bleu_score                          0.153684   \n",
       "embeddings_r2_recall                           0.551241   \n",
       "embeddings_r2_precision                         0.24712   \n",
       "embeddings_r2_f1                               0.321197   \n",
       "time_taken                                     1.299471   \n",
       "impossible_score                               0.413793   \n",
       "\n",
       "                                                     2  \n",
       "collection_name                legal_docs_voiyage2_512  \n",
       "Mean Reciprocal Rank (MRR)                    0.041667  \n",
       "Precision at 5 (P@5)                          0.041667  \n",
       "llm_gt_containment_proportion                      0.0  \n",
       "llm_unigram_match                                  0.5  \n",
       "llm_bigram_match                              0.348958  \n",
       "llm_trigram_match                             0.145833  \n",
       "llm_precision                                 0.258761  \n",
       "llm_recall                                    0.162771  \n",
       "llm_f1_score                                  0.182372  \n",
       "llm_r1_recall                                 0.161607  \n",
       "llm_r1_precision                              0.121734  \n",
       "llm_r1_f1                                     0.129176  \n",
       "embeddings_bleu_score                         0.025958  \n",
       "embeddings_r2_recall                          0.599061  \n",
       "embeddings_r2_precision                       0.087567  \n",
       "embeddings_r2_f1                              0.143358  \n",
       "time_taken                                    1.129063  \n",
       "impossible_score                              0.327586  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_summary = pd.read_parquet(f\"{BASE_DIR}/output/chunk_engineering/method-002/summary.parquet\")\n",
    "test_summary.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "\n",
    "def get_long_keywords():\n",
    "    with open(f'{BASE_DIR}/steer/question_keywords_long_answers.json', 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def preprocess_question(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def contains_keywords(question, keywords):\n",
    "    question_tokens = set(preprocess_question(question)) \n",
    "    return any(keyword in question_tokens for keyword in keywords)\n",
    "\n",
    "def retrieve_answer(question, n_results, document_title, collection):\n",
    "    question_embedding = call_vo_embeddings([question])\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=question_embedding.embeddings,\n",
    "        n_results=n_results,  # Return the closest match based on the number of results\n",
    "        where={\"doc_id\": document_title}  # Filter by document title\n",
    "    )\n",
    "    \n",
    "    context = results[\"documents\"][0][0] if results[\"documents\"] else None\n",
    "    context_embedding = collection.get(ids=results['ids'][0], include=['embeddings'])['embeddings']\n",
    "\n",
    "    if context:\n",
    "        print(f\"Context tokens: {count_tokens(context)}\")\n",
    "        prompt = f\"Your job is to answer legal questions in a succinct manner, providing only the essential information without any unnecessary preamble or additional commentary.\\nIf you cannot find the answer or you lack information to answer the question write 'Answer is impossible'.\\nHere is the question you need to answer:\\n<question>\\n{question}\\n</question>\\n\\nContext:\\n{context}\\n\\nPlease provide a direct and concise answer to this question. Focus solely on the most relevant legal information or response.\\n\\nFormat your response as a JSON object with a single key \\\"answer\\\" whose value is an array containing your response as a string. For example:\\n\\n{{\\n\\\"answer\\\": ['INTELLECTUAL PROPERTY AGREEMENT']\\n}}\\Or:\\n{{\\n\\\"answer\\\": ['Answer is impossible.']}}\\nOr:\\n{{\\n\\\"answer\\\": ['CERENCE INC.', 'SpinCo', 'Nuance', 'NUANCE COMMUNICATIONS, INC.']}}\\nOr:\\n{{\\n\\\"answer\\\": ['September 30, 2019']}}\"\n",
    "\n",
    "        print(f\"Prompt + context tokens: {count_tokens(prompt)}\")\n",
    "        answer = call_claude(prompt)\n",
    "        answer = answer[0].text\n",
    "        \n",
    "        return answer, context, context_embedding\n",
    "    else:\n",
    "        print(f\"No relevant chunk found for document {document_title}.\")\n",
    "        return None, None, None\n",
    "    \n",
    "def find_gt_chunk(chunks, answer_starts):\n",
    "    current_pos = 0\n",
    "    valid_chunks = []  # To collect all valid chunks for different answer_start\n",
    "\n",
    "    # Loop through all answer starts\n",
    "    for answer_start in answer_starts:\n",
    "        for chunk in chunks:\n",
    "            chunk_length = len(chunk)\n",
    "\n",
    "            if answer_start <= chunk_length:\n",
    "                valid_chunks.append(chunk)  # If answer_start is within this chunk, keep it\n",
    "                break  # No need to check further chunks for this answer_start\n",
    "            else:\n",
    "                current_pos += chunk_length\n",
    "\n",
    "                if answer_start <= current_pos:\n",
    "                    valid_chunks.append(chunk)  # Found the chunk with the answer_start\n",
    "                    break\n",
    "\n",
    "    return list(set(valid_chunks))  # Return all valid chunks that could match\n",
    "\n",
    "def run_faq_rag(data, collection_name):\n",
    "    collection = db_client.get_or_create_collection(name=collection_name)\n",
    "    print(f\"Created ChromaDB collection: {collection_name}\")\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for document in data[:1]:\n",
    "        document_title = document[\"title\"]  # Store document title for relevant retrieval\n",
    "        for paragraph in document[\"paragraphs\"][:1]:\n",
    "            context = paragraph[\"context\"]\n",
    "            \n",
    "            token_count = int(collection_name.split('_')[-1])\n",
    "            chunks = chunk_text(context, max_tokens=token_count)\n",
    "\n",
    "            for qa in paragraph[\"qas\"][:2]:\n",
    "                question = qa[\"question\"]\n",
    "                gt_answers = [answer[\"text\"] for answer in qa[\"answers\"]]\n",
    "                gt_answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]] \n",
    "                gt_is_impossible = qa.get(\"is_impossible\", False)\n",
    "\n",
    "                if not gt_is_impossible:  # Should we filter for impossible to answer for all\n",
    "                    gt_contexts = find_gt_chunk(chunks, gt_answer_starts)\n",
    "                    gt_context_embedding = call_vo_embeddings(gt_contexts).embeddings[0]\n",
    "                else:\n",
    "                    gt_contexts = []\n",
    "                    gt_context_embedding = []\n",
    "\n",
    "                if contains_keywords(question, get_long_keywords()):\n",
    "                    n_results = 5  # Retrieve more documents for long-answer questions\n",
    "                else:\n",
    "                    n_results = 1  # Retrieve only one document for short-answer questions\n",
    "\n",
    "                start_time = time.time()\n",
    "                answer, best_chunk, best_chunk_embedding = retrieve_answer(question, n_results, document_title, collection)\n",
    "                end_time = time.time()\n",
    "                time_taken = end_time - start_time\n",
    "\n",
    "                dfs.append({\n",
    "                    \"document_title\": document_title,\n",
    "                    \"question\": question,\n",
    "                    \"context\": best_chunk,\n",
    "                    \"context_embedding\": best_chunk_embedding,\n",
    "                    \"answer\": answer,\n",
    "                    \"gt_contexts\": gt_contexts,\n",
    "                    \"gt_context_embedding\": gt_context_embedding,\n",
    "                    \"gt_answer_starts\": gt_answer_starts,\n",
    "                    \"gt_answers\": gt_answers,\n",
    "                    \"gt_is_impossible\": gt_is_impossible,\n",
    "                    \"time_taken\": time_taken\n",
    "                })\n",
    "    df = pd.DataFrame(dfs)\n",
    "    df[\"collection_name\"] = collection_name\n",
    "    return df\n",
    "\n",
    "client = chromadb.PersistentClient(path=f\"{BASE_DIR}/embeddings/voiyage-2\")\n",
    "collections = client.list_collections()\n",
    "available_collections = [c.name for c in collections]\n",
    "if 'legal_docs_voiyage2' in available_collections:\n",
    "    available_collections.remove('legal_docs_voiyage2')\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for collection_name in available_collections:\n",
    "    df = run_faq_rag(data, collection_name)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output / Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import evaluate_llm_response, fuzzmatch_llm_scoring, evaluate_context_response\n",
    "\n",
    "def evaluate_rag(df):\n",
    "    is_possible = df[df.gt_is_impossible == False]\n",
    "    print(is_possible.shape)\n",
    "    impossible = df[df.gt_is_impossible == True]\n",
    "    print(impossible.shape)\n",
    "\n",
    "    apply_llm_similarity_evaluation(is_possible, 'llm', 'gt_answers', 'answer')\n",
    "    is_possible = fuzzmatch_llm_scoring(is_possible, threshold=80)\n",
    "    apply_context_similarity_evaluation(is_possible, 'embeddings', 'gt_contexts', 'context')\n",
    "\n",
    "    # Group by 'collection_name' and calculate mean for most metrics, but proportion for llm_gt_containment\n",
    "    summary_df = is_possible.groupby('collection_name').agg({\n",
    "        'llm_gt_containment': 'mean',  # This will give the proportion of True values\n",
    "        'llm_unigram_match': 'mean',\n",
    "        'llm_bigram_match': 'mean',\n",
    "        'llm_trigram_match': 'mean',\n",
    "        'llm_precision': 'mean',\n",
    "        'llm_recall': 'mean',\n",
    "        'llm_f1_score': 'mean',\n",
    "        'llm_r1_recall': 'mean',\n",
    "        'llm_r1_precision': 'mean',\n",
    "        'llm_r1_f1': 'mean',\n",
    "        'embeddings_bleu_score': 'mean',\n",
    "        'embeddings_r2_recall': 'mean',\n",
    "        'embeddings_r2_precision': 'mean',\n",
    "        'embeddings_r2_f1': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    impossible_score = impossible.groupby('collection_name').agg({\n",
    "        'answer': lambda x: (x == 'Answer is impossible.').mean()\n",
    "    }).reset_index().rename(columns={'answer': 'impossible_score'})\n",
    "\n",
    "    summary_df = pd.merge(summary_df, impossible_score, on='collection_name', how='left')\n",
    "\n",
    "    summary_df = summary_df.rename(columns={'llm_gt_containment': 'llm_gt_containment_proportion'})\n",
    "\n",
    "    return summary_df\n",
    "evaluate_rag(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 12)\n"
     ]
    }
   ],
   "source": [
    "test_main = pd.read_parquet(f\"{BASE_DIR}/output/main/test-001/data.parquet\")\n",
    "print(test_main.shape)\n",
    "# test_main.head(2)\n",
    "\n",
    "test_summary = pd.read_parquet(f\"{BASE_DIR}/output/chunk_engineering/test-001/summary.parquet\")\n",
    "test_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Unigram Match: 40%\n",
      "Average Bigram Match: 31%\n",
      "Average Trigram Match: 29%\n"
     ]
    }
   ],
   "source": [
    "from eval import apply_llm_similarity_evaluation, apply_gt_containment_evaluation, fuzzmatch_llm_scoring, apply_context_similarity_evaluation\n",
    "\n",
    "def mean_reciprocal_rank(df, k=5):\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank (MRR) for the top k retrieved results.\n",
    "    :param df: DataFrame containing the relevant data (e.g., relevance scores, context).\n",
    "    :param k: Number of top results to consider.\n",
    "    :return: MRR score.\n",
    "    \"\"\"\n",
    "    mrr_scores = []\n",
    "    \n",
    "    # Ensure that 'gt_relevance' is in the DataFrame passed to this function\n",
    "    if 'gt_relevance' not in df.columns:\n",
    "        raise KeyError(\"'gt_relevance' column is missing from the DataFrame.\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Since 'gt_relevance' is expected to be an integer, just check its value directly\n",
    "        relevance = row['gt_relevance']\n",
    "        \n",
    "        if relevance == 1:  # If the relevance score is 1, consider it relevant\n",
    "            mrr_scores.append(1)  # Reciprocal rank for relevant document\n",
    "        else:\n",
    "            mrr_scores.append(0)  # If not relevant, reciprocal rank is 0.\n",
    "    \n",
    "    return sum(mrr_scores) / len(mrr_scores)\n",
    "\n",
    "def precision_at_k(df, k=5):\n",
    "    \"\"\"\n",
    "    Calculate Precision at k (P@k) for the top k retrieved results.\n",
    "    :param df: DataFrame containing the relevant data (e.g., relevance scores, context).\n",
    "    :param k: Number of top results to consider.\n",
    "    :return: Precision at k score.\n",
    "    \"\"\"\n",
    "    precision_scores = []\n",
    "    \n",
    "    # Ensure that 'gt_relevance' is in the DataFrame passed to this function\n",
    "    if 'gt_relevance' not in df.columns:\n",
    "        raise KeyError(\"'gt_relevance' column is missing from the DataFrame.\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Get the top k relevance scores for each query (no need to iterate if it's just a binary score)\n",
    "        top_k_relevance = row['gt_relevance']  # We expect a single value per row, not a list\n",
    "        precision = top_k_relevance  # Just using the value directly since it's binary\n",
    "        precision_scores.append(precision)\n",
    "    \n",
    "    return sum(precision_scores) / len(precision_scores)\n",
    "\n",
    "def calculate_gt_relevance(df, threshold):\n",
    "    \"\"\"\n",
    "    Calculate a relevance score based on the BLEU score or other similarity metrics.\n",
    "    - If the BLEU score or other metric exceeds a threshold, it is marked as relevant (1).\n",
    "    - Otherwise, it is marked as non-relevant (0).\n",
    "    \"\"\"\n",
    "    # Ensure the 'embeddings_bleu_score' column exists\n",
    "    if 'embeddings_bleu_score' not in df.columns:\n",
    "        raise KeyError(\"'embeddings_bleu_score' column is missing from the DataFrame.\")\n",
    "    \n",
    "    df['gt_relevance'] = df['embeddings_bleu_score'].apply(lambda x: 1 if x >= threshold else 0)\n",
    "    return df\n",
    "\n",
    "def evaluate_rag_with_ranking(df, k=5, threshold=0.1):  # Lower threshold\n",
    "    is_possible = df[df.gt_is_impossible == False]\n",
    "    impossible = df[df.gt_is_impossible == True]\n",
    "    \n",
    "    # Apply evaluations\n",
    "    apply_llm_similarity_evaluation(is_possible, 'llm', 'gt_answers', 'answer')\n",
    "    apply_gt_containment_evaluation(is_possible, 'gt_answers', 'answer')\n",
    "    is_possible = fuzzmatch_llm_scoring(is_possible, threshold=80)\n",
    "    apply_context_similarity_evaluation(is_possible, 'embeddings', 'gt_contexts', 'context')\n",
    "\n",
    "    # Calculate gt_relevance based on the similarity score\n",
    "    is_possible = calculate_gt_relevance(is_possible, threshold)\n",
    "\n",
    "    # Initialize an empty list to collect results for each collection\n",
    "    results = []\n",
    "\n",
    "    # Group by collection_name to calculate per-collection metrics\n",
    "    for collection, group in is_possible.groupby('collection_name'):\n",
    "        # Calculate MRR and Precision at k for this collection\n",
    "        mrr_score = mean_reciprocal_rank(group, k)\n",
    "        precision_score = precision_at_k(group, k)\n",
    "\n",
    "        # Create a dictionary to store the results for this collection\n",
    "        collection_results = {\n",
    "            'collection_name': collection,\n",
    "            'Mean Reciprocal Rank (MRR)': mrr_score,\n",
    "            'Precision at 5 (P@5)': precision_score,\n",
    "            'llm_gt_containment_proportion': group['llm_gt_containment'].mean(),\n",
    "            'llm_unigram_match': group['llm_unigram_match'].mean(),\n",
    "            'llm_bigram_match': group['llm_bigram_match'].mean(),\n",
    "            'llm_trigram_match': group['llm_trigram_match'].mean(),\n",
    "            'llm_precision': group['llm_precision'].mean(),\n",
    "            'llm_recall': group['llm_recall'].mean(),\n",
    "            'llm_f1_score': group['llm_f1_score'].mean(),\n",
    "            'llm_r1_recall': group['llm_r1_recall'].mean(),\n",
    "            'llm_r1_precision': group['llm_r1_precision'].mean(),\n",
    "            'llm_r1_f1': group['llm_r1_f1'].mean(),\n",
    "            'embeddings_bleu_score': group['embeddings_bleu_score'].mean(),\n",
    "            'embeddings_r2_recall': group['embeddings_r2_recall'].mean(),\n",
    "            'embeddings_r2_precision': group['embeddings_r2_precision'].mean(),\n",
    "            'embeddings_r2_f1': group['embeddings_r2_f1'].mean(),\n",
    "            'time_taken': group['time_taken'].mean(),  # Include time taken for inference\n",
    "        }\n",
    "\n",
    "        # Calculate the impossible score for this collection\n",
    "        impossible_subset = impossible[impossible['collection_name'] == collection]\n",
    "        \n",
    "        if not impossible_subset.empty:\n",
    "            impossible_score = (impossible_subset['answer'] == 'Answer is impossible.').mean()\n",
    "        else:\n",
    "            impossible_score = 0.0  # If no impossible answers for this collection\n",
    "        \n",
    "        collection_results['impossible_score'] = impossible_score\n",
    "\n",
    "        # Add this collection's results to the list\n",
    "        results.append(collection_results)\n",
    "\n",
    "    # Convert the list of results into a DataFrame\n",
    "    summary_df = pd.DataFrame(results)\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "out = evaluate_rag_with_ranking(test_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.to_parquet(f\"{BASE_DIR}/output/chunk_engineering/test-001/summary.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth Chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics:\n",
      "Question Count: 820\n",
      "Answer Count: 820\n",
      "None Answer Count: 0\n",
      "Average Question Token Length: 42.02439024390244\n",
      "Median Answer Length: 1\n",
      "Max Distances (if > 10 answers): [1661, 2328, 47334, 814]\n",
      "Average Answer Start Distance from Context Start: 21914.410053258147\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "from embeddings import chunk_text\n",
    "\n",
    "def calculate_max_distance(answer_starts):\n",
    "    # Sort the answer start positions\n",
    "    sorted_starts = sorted(answer_starts)\n",
    "    \n",
    "    # Calculate the differences between consecutive answer starts\n",
    "    differences = [sorted_starts[i+1] - sorted_starts[i] for i in range(len(sorted_starts) - 1)]\n",
    "    \n",
    "    # Return the maximum distance\n",
    "    return max(differences) if differences else 0  # Handle the case of only one answer\n",
    "\n",
    "def find_gt_chunk(chunks, answer_starts):\n",
    "    current_pos = 0\n",
    "    valid_chunks = []  # To collect all valid chunks for different answer_start\n",
    "\n",
    "    # Loop through all answer starts\n",
    "    for answer_start in answer_starts:\n",
    "        for chunk in chunks:\n",
    "            chunk_length = len(chunk)\n",
    "\n",
    "            if answer_start <= chunk_length:\n",
    "                valid_chunks.append(chunk)  # If answer_start is within this chunk, keep it\n",
    "                break  # No need to check further chunks for this answer_start\n",
    "            else:\n",
    "                current_pos += chunk_length\n",
    "\n",
    "                if answer_start <= current_pos:\n",
    "                    valid_chunks.append(chunk)  # Found the chunk with the answer_start\n",
    "                    break\n",
    "\n",
    "    return list(set(valid_chunks))  # Return all valid chunks that could match\n",
    "\n",
    "question_count = 0\n",
    "answer_count = 0\n",
    "none_answer_count = 0\n",
    "answer_lengths = []\n",
    "max_distances_starts = []\n",
    "question_token_lengths = []\n",
    "answer_start_distances = []\n",
    "question_answer_dict = {}\n",
    "\n",
    "for document in data:\n",
    "    document_title = document[\"title\"]  \n",
    "    for paragraph in document[\"paragraphs\"]:\n",
    "        context = paragraph[\"context\"]\n",
    "\n",
    "        token_count = 4096\n",
    "        chunks = chunk_text(context, max_tokens=token_count)\n",
    "\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            question = qa[\"question\"]\n",
    "            if len(question) > 3:\n",
    "                question_count += 1\n",
    "                question_token_lengths.append(len(question.split()))  # Token length of the question\n",
    "            else:\n",
    "                print(question)\n",
    "\n",
    "            gt_is_impossible = qa.get(\"is_impossible\", False)\n",
    "            if not gt_is_impossible: \n",
    "                # Ensure gt_answers is always a list of strings, even if only one answer exists\n",
    "                gt_answers = [answer[\"text\"] for answer in qa[\"answers\"]]\n",
    "                gt_answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]] \n",
    "                answer_lengths.append(len(gt_answers))\n",
    "\n",
    "                if len(gt_answers) > 10:\n",
    "                    max_distances = calculate_max_distance(gt_answer_starts)\n",
    "                    max_distances_starts.append(max_distances)\n",
    "\n",
    "                # Store answers as a list of strings for each question\n",
    "                question_answer_dict[question] = gt_answers\n",
    "            \n",
    "            try:\n",
    "                valid_chunks = find_gt_chunk(chunks, gt_answer_starts)  \n",
    "                if valid_chunks:\n",
    "                    answer_count += 1\n",
    "                    # Calculate distance from the start of the context to the answer start\n",
    "                    answer_start_distance = statistics.mean(gt_answer_starts) if gt_answer_starts else 0\n",
    "                    answer_start_distances.append(answer_start_distance)\n",
    "                else:\n",
    "                    if qa['is_impossible'] == False:\n",
    "                        none_answer_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing answer_start {gt_answer_starts}: {e}\")\n",
    "                none_answer_count += 1\n",
    "\n",
    "# Statistical Analysis\n",
    "print(\"Statistics:\")\n",
    "print(f\"Question Count: {question_count}\")\n",
    "print(f\"Answer Count: {answer_count}\")\n",
    "print(f\"None Answer Count: {none_answer_count}\")\n",
    "print(f\"Average Question Token Length: {statistics.mean(question_token_lengths)}\")\n",
    "print(f\"Median Answer Length: {statistics.median(answer_lengths)}\")\n",
    "print(f\"Max Distances (if > 10 answers): {max_distances_starts}\")\n",
    "print(f\"Average Answer Start Distance from Context Start: {statistics.mean(answer_start_distances)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Analysis in Long Answer/Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "\n",
    "questions = list(question_answer_dict.keys())\n",
    "answers = [' '.join(answer) for answer in question_answer_dict.values()]\n",
    "qa_df = pd.DataFrame({'question': questions, 'answer': answers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = list(question_answer_dict.keys())\n",
    "answers = list(question_answer_dict.values())  # Keep answers as lists\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "qa_df = pd.DataFrame({'question': questions, 'answer': answers})\n",
    "qa_df['answer_length'] = qa_df['answer'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify top 25% longest answers\n",
    "long_answers = qa_df['answer_length'].quantile(0.75)\n",
    "long_answer_df = qa_df[qa_df['answer_length'] > long_answers]\n",
    "single_answer_df = qa_df[qa_df['answer_length'] == 1]\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to questions\n",
    "qa_df['preprocessed_question'] = qa_df['question'].apply(preprocess_text)\n",
    "long_answer_df['preprocessed_question'] = long_answer_df['question'].apply(preprocess_text)\n",
    "single_answer_df['preprocessed_question'] = single_answer_df['question'].apply(preprocess_text)\n",
    "\n",
    "# Word frequency analysis\n",
    "all_question_words = [word for words in qa_df['preprocessed_question'] for word in words]\n",
    "long_answer_words = [word for words in long_answer_df['preprocessed_question'] for word in words]\n",
    "single_answer_words = [word for words in single_answer_df['preprocessed_question'] for word in words]\n",
    "\n",
    "all_question_freq = Counter(all_question_words)\n",
    "long_answer_freq = Counter(long_answer_words)\n",
    "single_answer_freq = Counter(single_answer_words)\n",
    "\n",
    "# Get unique keywords for questions with long answers not in short answers\n",
    "unique_question_keywords_long_answers = {word: count for word, count in long_answer_freq.items() if word not in single_answer_freq}\n",
    "# Get unique keywords for questions with short answers not in long answers\n",
    "unique_question_keywords_short_answers = {word: count for word, count in single_answer_freq.items() if word not in long_answer_freq}\n",
    "\n",
    "# Sort the keywords by frequency in descending order\n",
    "unique_question_keywords_long_answers = sorted(unique_question_keywords_long_answers.items(), key=lambda x: x[1], reverse=True)\n",
    "unique_question_keywords_long_answers = [w[0] for w in unique_question_keywords_long_answers]\n",
    "unique_question_keywords_short_answers = sorted(unique_question_keywords_short_answers.items(), key=lambda x: x[1], reverse=True)\n",
    "unique_question_keywords_short_answers = [w[0] for w in unique_question_keywords_short_answers]\n",
    "\n",
    "# Write the sorted keywords to JSON files\n",
    "with open(f'{BASE_DIR}/steer/question_keywords_short_answers.json', 'w') as f:\n",
    "    json.dump(unique_question_keywords_short_answers, f, indent=4)\n",
    "\n",
    "with open(f'{BASE_DIR}/steer/question_keywords_long_answers.json', 'w') as f:\n",
    "    json.dump(unique_question_keywords_long_answers, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_answers = max(answer_lengths)  \n",
    "avg_answers = np.mean(answer_lengths)  \n",
    "median_answers = np.median(answer_lengths) \n",
    "num_questions = len(answer_lengths)  \n",
    "\n",
    "# Answer Length Distribution (Histogram)\n",
    "plt.hist(answer_lengths, bins=range(0, max_answers + 2), edgecolor='black')\n",
    "plt.title('Distribution of Answer Lengths')\n",
    "plt.xlabel('Number of Answers')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import evaluate_response\n",
    "\n",
    "# IS POSSIBLE\n",
    "\n",
    "def apply_text_similarity_evaluation(df,  prefix, expected_col, generated_col):\n",
    "    df[[f'{prefix}_gt_containment', f'{prefix}_precision', f'{prefix}_recall', f'{prefix}_f1_score', f'{prefix}_bleu_score', f'{prefix}_r1_recall', f'{prefix}_r1_precision', f'{prefix}_r1_f1', f'{prefix}_r2_recall', f'{prefix}_r2_precision', f'{prefix}_r2_f1', f'{prefix}_rl_recall', f'{prefix}_rl_precision', f'{prefix}_rl_f1']] = df.apply(\n",
    "        lambda x: pd.Series(evaluate_response(x[expected_col], x[generated_col])), axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "Where answer is_possible evaluate ground truth answers with answer\n",
    "\"\"\"\n",
    "apply_text_similarity_evaluation(is_possible, 'llm', 'gt_answers', 'answer')\n",
    "\n",
    "\"\"\"\n",
    "Where answer is_possible compare similarity of ground truth context with used context\n",
    "\"\"\"\n",
    "apply_text_similarity_evaluation(is_possible, 'embeddings', 'gt_contexts', 'context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(f\"{BASE_DIR}/output/chunk_engineering/method-001/summary.parquet\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_count = 0\n",
    "\n",
    "for document in data[:1]:\n",
    "    document_title = document[\"title\"]  # Store document title for relevant retrieval\n",
    "    for paragraph in document[\"paragraphs\"][:2]:\n",
    "        context = paragraph[\"context\"]\n",
    "        \n",
    "        token_count = int(collection_name.split('_')[-1])\n",
    "        chunks = chunk_text(context, max_tokens=token_count)\n",
    "\n",
    "        for qa in paragraph[\"qas\"][:5]:\n",
    "            print(qa.get(\"is_impossible\", False))\n",
    "            qa_count+=1\n",
    "\n",
    "print(qa_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzz match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Where answer is_possible check if the answer is in the context \n",
    "\"\"\"\n",
    "\n",
    "from rapidfuzz import fuzz\n",
    "import re\n",
    "\n",
    "def match_proportion(gt_answers, generated_answer, threshold=80):\n",
    "    \"\"\"Calculate the proportion of ground truth tokens found in the generated answer with fuzzy matching.\"\"\"\n",
    "    matches = 0\n",
    "    \n",
    "    # Tokenize and clean up gt_answers for partial match flexibility\n",
    "    tokenized_gt_answers = [re.sub(r\"[^\\w\\s]\", \"\", answer).lower().split() for answer in gt_answers]\n",
    "    generated_tokens = re.sub(r\"[^\\w\\s]\", \"\", generated_answer).lower().split()\n",
    "\n",
    "    # For each set of tokens in gt_answers\n",
    "    for answer_tokens in tokenized_gt_answers:\n",
    "        token_match_count = 0\n",
    "        \n",
    "        for token in answer_tokens:\n",
    "            # Check if each token from the ground truth answer has a fuzzy match in the generated answer\n",
    "            match_found = False\n",
    "            for gen_token in generated_tokens:\n",
    "                similarity = fuzz.partial_ratio(token, gen_token)\n",
    "                if similarity >= threshold:\n",
    "                    token_match_count += 1\n",
    "                    match_found = True\n",
    "                    break  # Token matched, no need to check other generated tokens for this token\n",
    "                \n",
    "            if not match_found:\n",
    "                print(f\"Debug: '{token}' from '{' '.join(answer_tokens)}' not found in generated answer: {generated_answer}.\")\n",
    "        \n",
    "        # If all tokens in the ground truth answer have fuzzy matches, count it as a match\n",
    "        if token_match_count == len(answer_tokens):\n",
    "            matches += 1\n",
    "    \n",
    "    # Calculate the proportion of gt_answers that met the similarity threshold\n",
    "    return matches / len(gt_answers) if gt_answers else 0\n",
    "\n",
    "# Calculate the percentage of ground truth answers present in the generated context and answer\n",
    "is_possible['context_match_perc'] = is_possible.apply(lambda row: match_proportion(row['gt_answers'], row['context']), axis=1)\n",
    "is_possible['answer_match_perc'] = is_possible.apply(lambda row: match_proportion(row['gt_answers'], row['answer']), axis=1)\n",
    "\n",
    "# Calculate the overall percentage by averaging across rows\n",
    "context_match_average = is_possible['context_match_perc'].mean() * 100\n",
    "answer_match_average = is_possible['answer_match_perc'].mean() * 100\n",
    "\n",
    "print(f\"Average Ground Truth Answer in Generated Context: {context_match_average:.0f}%\")\n",
    "print(f\"Average Ground Truth Answer in Generated Answer: {answer_match_average:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram, Bigram, Trigram Answer FuzzMatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from rapidfuzz import fuzz\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def generate_ngrams(text, n=1):\n",
    "    \"\"\"Generate n-grams (unigrams, bigrams, trigrams) from text.\"\"\"\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text).lower()\n",
    "    if not text or text == 'answer is impossible.' or not text.strip():  # Handle empty or invalid text\n",
    "        return []  # Return empty if text is invalid\n",
    "    # Create n-grams using CountVectorizer\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), analyzer='word', stop_words='english')\n",
    "    try:\n",
    "        # Fit the vectorizer to the text and transform it\n",
    "        ngrams_matrix = vectorizer.fit_transform([text])\n",
    "        # Get the list of n-grams (features)\n",
    "        ngram_list = vectorizer.get_feature_names_out()\n",
    "        return ngram_list\n",
    "    except ValueError:\n",
    "        return []  # Handle empty vocabulary error\n",
    "    \n",
    "def match_fuzzy_ngram(gt_answers, generated_answer, ngram_type='unigram', threshold=80):\n",
    "    \"\"\"Match fuzzy n-grams from generated_answer against ground truth answers.\"\"\"\n",
    "    matches = 0\n",
    "\n",
    "    # Ensure gt_answers is a list or series and handle accordingly\n",
    "    if isinstance(gt_answers, str):\n",
    "        gt_answers = [gt_answers]  # Convert single string to a list\n",
    "    elif isinstance(gt_answers, np.ndarray) or isinstance(gt_answers, pd.Series):\n",
    "        gt_answers = gt_answers.tolist()  # Convert array or series to a list\n",
    "\n",
    "    # Generate n-grams for the generated answer (unigrams, bigrams, trigrams)\n",
    "    if ngram_type == 'unigram':\n",
    "        ngrams = generate_ngrams(generated_answer, 1)\n",
    "    elif ngram_type == 'bigram':\n",
    "        ngrams = generate_ngrams(generated_answer, 2)\n",
    "    elif ngram_type == 'trigram':\n",
    "        ngrams = generate_ngrams(generated_answer, 3)\n",
    "\n",
    "    # Check if ngrams is empty (for both list and numpy array types)\n",
    "    if isinstance(ngrams, (list, np.ndarray)) and len(ngrams) == 0:\n",
    "        return 0  # No matches if no n-grams are found\n",
    "\n",
    "    for gt_answer in gt_answers:\n",
    "        match_found = False\n",
    "        gt_clean = re.sub(r\"[^\\w\\s]\", \"\", gt_answer).lower()\n",
    "\n",
    "        for ngram in ngrams:\n",
    "            similarity = fuzz.partial_ratio(gt_clean, ngram)\n",
    "            if similarity >= threshold:\n",
    "                match_found = True\n",
    "                break  # We only need one fuzzy match for this gt_answer\n",
    "\n",
    "        if match_found:\n",
    "            matches += 1\n",
    "        # else: # uncomment for debugging\n",
    "        #     print(f\"The term {gt_answer} was not found in generated {generated_answer}\")\n",
    "\n",
    "    # Check if there are any gt_answers\n",
    "    if len(gt_answers) > 0:\n",
    "        return matches / len(gt_answers)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def fuzzmatch_llm_scoring(is_possible, threshold=80):\n",
    "    \"\"\"Evaluate match proportions for unigrams, bigrams, and trigrams.\"\"\"\n",
    "    # Evaluate for unigrams, bigrams, and trigrams\n",
    "    is_possible['llm_unigram_match'] = is_possible.apply(lambda row: match_fuzzy_ngram(row['gt_answers'], row['answer'], ngram_type='unigram', threshold=threshold), axis=1)\n",
    "    is_possible['llm_bigram_match'] = is_possible.apply(lambda row: match_fuzzy_ngram(row['gt_answers'], row['answer'], ngram_type='bigram', threshold=threshold), axis=1)\n",
    "    is_possible['llm_trigram_match'] = is_possible.apply(lambda row: match_fuzzy_ngram(row['gt_answers'], row['answer'], ngram_type='trigram', threshold=threshold), axis=1)\n",
    "\n",
    "    # Calculate average match rates\n",
    "    unigram_match_avg = is_possible['llm_unigram_match'].mean() * 100\n",
    "    bigram_match_avg = is_possible['llm_bigram_match'].mean() * 100\n",
    "    trigram_match_avg = is_possible['llm_trigram_match'].mean() * 100\n",
    "\n",
    "    print(f\"Average Unigram Match: {unigram_match_avg:.0f}%\")\n",
    "    print(f\"Average Bigram Match: {bigram_match_avg:.0f}%\")\n",
    "    print(f\"Average Trigram Match: {trigram_match_avg:.0f}%\")\n",
    "    \n",
    "    return is_possible\n",
    "\n",
    "out = pd.read_parquet(f\"{BASE_DIR}/output/main/method-001/data.parquet\")\n",
    "is_possible = out[out.gt_is_impossible == False]\n",
    "fuzzmatch_llm_scoring(is_possible, threshold=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "def generate_ngrams(text, n=1):\n",
    "    \"\"\"Generate n-grams (unigrams, bigrams, trigrams) from text.\"\"\"\n",
    "    # Clean up text\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text).lower()\n",
    "    # Create n-grams using CountVectorizer\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), analyzer='word')\n",
    "    ngrams = vectorizer.fit_transform([text])\n",
    "    ngram_list = vectorizer.get_feature_names_out()\n",
    "    return ngram_list\n",
    "\n",
    "def match_fuzzy_ngram(gt_answers, generated_answer, ngram_type='unigram', threshold=80):\n",
    "    \"\"\"Match fuzzy n-grams from generated_answer against ground truth answers.\"\"\"\n",
    "    matches = 0\n",
    "\n",
    "    # Generate n-grams for the generated answer (unigrams, bigrams, trigrams)\n",
    "    if ngram_type == 'unigram':\n",
    "        ngrams = generate_ngrams(generated_answer, 1)\n",
    "    elif ngram_type == 'bigram':\n",
    "        ngrams = generate_ngrams(generated_answer, 2)\n",
    "    elif ngram_type == 'trigram':\n",
    "        ngrams = generate_ngrams(generated_answer, 3)\n",
    "\n",
    "    for gt_answer in gt_answers:\n",
    "        match_found = False\n",
    "        # Clean and process each gt_answer for comparison\n",
    "        gt_clean = re.sub(r\"[^\\w\\s]\", \"\", gt_answer).lower()\n",
    "\n",
    "        # Check each n-gram in generated answer\n",
    "        for ngram in ngrams:\n",
    "            similarity = fuzz.partial_ratio(gt_clean, ngram)\n",
    "            if similarity >= threshold:\n",
    "                match_found = True\n",
    "                break  # We only need one fuzzy match for this gt_answer\n",
    "\n",
    "        if match_found:\n",
    "            matches += 1\n",
    "        else:\n",
    "            print(f\"Debug: '{gt_answer}' did not match any n-gram in generated answer: {generated_answer}.\")\n",
    "    \n",
    "    return matches / len(gt_answers) if gt_answers else 0\n",
    "\n",
    "\n",
    "def evaluate_matches(is_possible, threshold=80):\n",
    "    \"\"\"Evaluate match proportions for unigrams, bigrams, and trigrams.\"\"\"\n",
    "    # Evaluate for unigrams, bigrams, and trigrams\n",
    "    is_possible['answer_unigram_match'] = is_possible.apply(lambda row: match_fuzzy_ngram(row['gt_answers'], row['answer'], ngram_type='unigram', threshold=threshold), axis=1)\n",
    "    is_possible['answer_bigram_match'] = is_possible.apply(lambda row: match_fuzzy_ngram(row['gt_answers'], row['answer'], ngram_type='bigram', threshold=threshold), axis=1)\n",
    "    is_possible['answer_trigram_match'] = is_possible.apply(lambda row: match_fuzzy_ngram(row['gt_answers'], row['answer'], ngram_type='trigram', threshold=threshold), axis=1)\n",
    "\n",
    "    # Calculate average match rates\n",
    "    unigram_match_avg = is_possible['answer_unigram_match'].mean() * 100\n",
    "    bigram_match_avg = is_possible['answer_bigram_match'].mean() * 100\n",
    "    trigram_match_avg = is_possible['answer_trigram_match'].mean() * 100\n",
    "\n",
    "    print(f\"Average Unigram Match: {unigram_match_avg:.0f}%\")\n",
    "    print(f\"Average Bigram Match: {bigram_match_avg:.0f}%\")\n",
    "    print(f\"Average Trigram Match: {trigram_match_avg:.0f}%\")\n",
    "    \n",
    "    return is_possible\n",
    "\n",
    "\n",
    "# Assuming 'is_possible' is your DataFrame\n",
    "is_possible = evaluate_matches(is_possible, threshold=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPOSSIBLE\n",
    "\"\"\"\n",
    "Where answer impossible evaluate that responses negate providing an answer\n",
    "\"\"\"\n",
    "\n",
    "impossible_accuracy = impossible[impossible.answer=='Answer is impossible.'].answer.count()/impossible.shape[0]\n",
    "print(f\"Accuracy in not answering impossible questions: {impossible_accuracy*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'collection_name' and calculate the metrics\n",
    "grouped = is_possible.groupby('collection_name')\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "additional_results = {\n",
    "    'collection_name': [],\n",
    "    'gt_answer_in_generated_context': [],\n",
    "    'gt_answer_in_generated_answer': [],\n",
    "    'accuracy_impossible': []\n",
    "}\n",
    "\n",
    "# Iterate through the grouped data\n",
    "for collection_name, group in grouped:\n",
    "    # Calculate ground truth in context\n",
    "    group['correct_context'] = group.apply(lambda row: check_in_truth(row['gt_answers'], row['context']), axis=1)\n",
    "    context_accuracy = group[group.correct_context == True].question.count() / group.shape[0]\n",
    "    \n",
    "    # Calculate ground truth in generated answer\n",
    "    group['correct_answer'] = group.apply(lambda row: check_in_truth(row['gt_answers'], row['answer']), axis=1)\n",
    "    answer_accuracy = group[group.correct_answer == True].question.count() / group.shape[0]\n",
    "    \n",
    "    # Calculate impossible accuracy\n",
    "    impossible_group = group[group['answer'] == 'Answer is impossible.']\n",
    "    impossible_accuracy = impossible_group.shape[0] / group.shape[0]\n",
    "    \n",
    "    # Append results to the dictionary\n",
    "    additional_results['collection_name'].append(collection_name)\n",
    "    additional_results['gt_answer_in_generated_context'].append(context_accuracy)\n",
    "    additional_results['gt_answer_in_generated_answer'].append(answer_accuracy)\n",
    "    additional_results['accuracy_impossible'].append(impossible_accuracy)\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "additional_df = pd.DataFrame(additional_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ROUGE-1 (Unigrams): Evaluating keyword extraction or basic content overlap (e.g., detecting whether important terms are captured).\n",
    "- ROUGE-2 (Bigrams): More sensitive to phrase overlaps and used in tasks like summarization where a model should understand multi-word concepts.\n",
    "- ROUGE-L (LCS): Used when the preservation of order and structure is important, like in machine translation, sentence generation, or document summarization.\n",
    "- ROUGE-W (Weighted LCS): When certain sections of the reference text (such as key information or longer subsequences) are more important than others, like in specialized summarization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'collection_name' and calculate mean for most metrics, but proportion for llm_gt_containment\n",
    "summary_df = is_possible.groupby('collection_name').agg({\n",
    "    # 'llm_gt_containment': 'mean',  # This will give the proportion of True values\n",
    "    'llm_precision': 'mean',\n",
    "    'llm_recall': 'mean',\n",
    "    'llm_f1_score': 'mean',\n",
    "    'llm_r1_recall': 'mean',\n",
    "    'llm_r1_precision': 'mean',\n",
    "    'llm_r1_f1': 'mean',\n",
    "    # 'embeddings_bleu_score': 'mean',\n",
    "    # 'embeddings_r2_recall': 'mean',\n",
    "    # 'embeddings_r2_precision': 'mean',\n",
    "    # 'embeddings_r2_f1': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename llm_gt_containment to indicate it’s a proportion\n",
    "summary_df = summary_df.rename(columns={'llm_gt_containment': 'llm_gt_containment_proportion'})\n",
    "\n",
    "# Merge the additional_df with the existing summary_df (assuming summary_df is defined)\n",
    "merged_df = pd.merge(summary_df, additional_df, on='collection_name')\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review answers\n",
    "\n",
    "example =3\n",
    "\n",
    "marked = is_possible.correct_answer.values[example]\n",
    "generated = is_possible.answer.values[example]\n",
    "expected = is_possible.gt_answers.values[example]\n",
    "\n",
    "print(\"Marked answer:\")\n",
    "print(marked)\n",
    "print(\"Generated Answer:\")\n",
    "print(generated)\n",
    "print(\"Ground truth Answer:\")\n",
    "print(expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(is_possible.context.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = out[out.question.str.contains(\"Document Name\")]\n",
    "for idx, row in temp.iterrows():\n",
    "    # Print the last 100 characters of each context for clarity\n",
    "    print(f\"Collection: {row['collection_name']}\")\n",
    "    print(f\"GT Context: {row['gt_contexts'][-100:]}... -> Context: {row['context'][-100:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[['collection_name','llm_gt_containment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def generate_fake_question_embedding(dim=1024):\n",
    "#     return np.random.rand(dim)  # Fake random embedding\n",
    "\n",
    "# results = collection.query(\n",
    "#     query_embeddings=generate_fake_question_embedding(),\n",
    "#     n_results=1,  # Return the closest match\n",
    "#     where={\"doc_id\": document_title}  # Filter by document title\n",
    "# )\n",
    "\n",
    "# # Extract the best chunk text and its stored embedding from ChromaDB results\n",
    "# best_chunk = results[\"documents\"][0][0] if results[\"documents\"] else None\n",
    "# best_chunk_embeddings = collection.get(ids=results['ids'][0], include=['embeddings'])['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view examples\n",
    "\n",
    "example = 1\n",
    "# print(\"ground truth\")\n",
    "# print(final_df['gt_context'].values[example][:100])\n",
    "# print(\"best chunk\")\n",
    "# print(final_df['context'].values[example][:100])\n",
    "\n",
    "print(\"ground truth\")\n",
    "print(out['gt_answers'].values[example][:100])\n",
    "print(\"answer\")\n",
    "print(out['answer'].values[example][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # view examples\n",
    "\n",
    "# example = 0\n",
    "# # print(\"ground truth\")\n",
    "# # print(final_df['gt_context'].values[example][:100])\n",
    "# # print(\"best chunk\")\n",
    "# # print(final_df['context'].values[example][:100])\n",
    "\n",
    "# print(\"ground truth\")\n",
    "# print(final_df['gt_answers'].values[example][:100])\n",
    "# print(\"answer\")\n",
    "# print(final_df['answer'].values[example][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collection_name</th>\n",
       "      <th>llm_gt_containment_proportion</th>\n",
       "      <th>llm_unigram_match</th>\n",
       "      <th>llm_bigram_match</th>\n",
       "      <th>llm_trigram_match</th>\n",
       "      <th>llm_precision</th>\n",
       "      <th>llm_recall</th>\n",
       "      <th>llm_f1_score</th>\n",
       "      <th>llm_r1_recall</th>\n",
       "      <th>llm_r1_precision</th>\n",
       "      <th>llm_r1_f1</th>\n",
       "      <th>embeddings_bleu_score</th>\n",
       "      <th>embeddings_r2_recall</th>\n",
       "      <th>embeddings_r2_precision</th>\n",
       "      <th>embeddings_r2_f1</th>\n",
       "      <th>impossible_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>legal_docs_voiyage2_1024</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.352823</td>\n",
       "      <td>0.341398</td>\n",
       "      <td>0.248520</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.083991</td>\n",
       "      <td>0.058279</td>\n",
       "      <td>0.058675</td>\n",
       "      <td>0.057936</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>legal_docs_voiyage2_2048</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.528898</td>\n",
       "      <td>0.390030</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.159524</td>\n",
       "      <td>0.108295</td>\n",
       "      <td>0.137127</td>\n",
       "      <td>0.085142</td>\n",
       "      <td>0.104801</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>legal_docs_voiyage2_512</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.381757</td>\n",
       "      <td>0.362231</td>\n",
       "      <td>0.323533</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.094424</td>\n",
       "      <td>0.052529</td>\n",
       "      <td>0.049270</td>\n",
       "      <td>0.050125</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            collection_name  llm_gt_containment_proportion  llm_unigram_match  \\\n",
       "0  legal_docs_voiyage2_1024                           0.25             0.4375   \n",
       "1  legal_docs_voiyage2_2048                           0.50             0.4375   \n",
       "2   legal_docs_voiyage2_512                           0.25             0.3125   \n",
       "\n",
       "   llm_bigram_match  llm_trigram_match  llm_precision  llm_recall  \\\n",
       "0            0.1875             0.1875       0.352823    0.341398   \n",
       "1            0.4375             0.3750       0.423077    0.528898   \n",
       "2            0.3125             0.3125       0.381757    0.362231   \n",
       "\n",
       "   llm_f1_score  llm_r1_recall  llm_r1_precision  llm_r1_f1  \\\n",
       "0      0.248520       0.006944          0.041667   0.011905   \n",
       "1      0.390030       0.177083          0.166667   0.159524   \n",
       "2      0.323533       0.166667          0.125000   0.142857   \n",
       "\n",
       "   embeddings_bleu_score  embeddings_r2_recall  embeddings_r2_precision  \\\n",
       "0               0.083991              0.058279                 0.058675   \n",
       "1               0.108295              0.137127                 0.085142   \n",
       "2               0.094424              0.052529                 0.049270   \n",
       "\n",
       "   embeddings_r2_f1  impossible_score  \n",
       "0          0.057936               1.0  \n",
       "1          0.104801               1.0  \n",
       "2          0.050125               1.0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import evaluate_response, evaluate_bleu\n",
    "\n",
    "\"\"\"\n",
    "ROUGE Variants:\n",
    "\n",
    "You have three ROUGE variants: ROUGE-1, ROUGE-2, and ROUGE-L. Each focuses on different aspects of text similarity.\n",
    "\n",
    "ROUGE-1 (Unigram-based):\n",
    "Evaluates the overlap of individual words between the generated and reference texts.\n",
    "ROUGE-2 (Bigram-based):\n",
    "Assesses the overlap of pairs of consecutive words (bigrams).\n",
    "ROUGE-L (Longest Common Subsequence):\n",
    "Measures the longest common subsequence of words, emphasizing fluency and word order.\n",
    "\n",
    "Metrics within Each ROUGE Variant:\n",
    "\n",
    "R (Recall): Measures the proportion of reference text's units (unigrams, bigrams, or longest common subsequence) found in the generated text.\n",
    "Higher is Better (up to 1.0)\n",
    "P (Precision): Calculates the proportion of generated text's units that are also found in the reference text.\n",
    "Higher is Better (up to 1.0)\n",
    "F (F1 Score): The harmonic mean of Precision and Recall, offering a balanced measure.\n",
    "Higher is Better (up to 1.0)\n",
    "\"\"\"\n",
    "\n",
    "evaluate_response(final_df['gt_context'].values[0],  final_df['context'].values[0])\n",
    "# evaluate_bleu(final_df['gt_context'].values[0],  final_df['context'].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.gt_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.context.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for columns that contain scored data (e.g., metrics or embeddings)\n",
    "score_cols = [c for c in evaluated_embeddings.columns if c.startswith('embeddings')]\n",
    "mean_scores = evaluated_embeddings[score_cols].mean()\n",
    "\n",
    "mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for columns that contain scored data (e.g., metrics or embeddings)\n",
    "score_cols = [c for c in evaluated_embeddings.columns if c.startswith('llm')]\n",
    "mean_scores = evaluated_answers[score_cols]\n",
    "# .mean()\n",
    "\n",
    "mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load a sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "final_df['context_embedding'] = final_df['context_used'].apply(lambda x: model.encode(x))\n",
    "final_df['gt_context_embedding'] = final_df['gt_context'].apply(lambda x: model.encode(x))\n",
    "\n",
    "# Calculate cosine similarity\n",
    "final_df['cosine_similarity'] = final_df.apply(\n",
    "    lambda x: cosine_similarity(\n",
    "        np.array(x['context_used_embedding']).reshape(1, -1),\n",
    "        np.array(x['gt_context_embedding']).reshape(1, -1)\n",
    "    )[0][0], axis=1\n",
    ")\n",
    "cosine_similarity_mean = final_df['cosine_similarity'].mean()\n",
    "print(f\"Mean Cosine Similarity: {cosine_similarity_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[['question','answer','gt_answers']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve answer using ChromaDB\n",
    "def retrieve_answer(question, document_title):\n",
    "    # Generate embedding for the question\n",
    "    question_embedding = call_vo_embeddings([question])\n",
    "    \n",
    "    # Query ChromaDB for the most similar chunk, filtering by document title\n",
    "    results = collection.query(\n",
    "        query_embeddings=question_embedding.embeddings,\n",
    "        n_results=1,  # Return the closest match\n",
    "       where={\"doc_id\": document_title}  # Filter by documents by title\n",
    "    )\n",
    "    \n",
    "    # Extract the best chunk text from ChromaDB results\n",
    "    best_chunk = results[\"documents\"][0][0] if results[\"documents\"] else None\n",
    "\n",
    "    if best_chunk:\n",
    "        print(f\"Current tokens: {count_tokens(best_chunk)}\")\n",
    "        \n",
    "        # Construct the prompt for the LLM, including the question and the most relevant chunk\n",
    "        prompt = f\"Question: {question}\\nContext: {best_chunk}\\nAnswer:\"\n",
    "        \n",
    "        temperature = 0.1\n",
    "        max_tokens = 7000\n",
    "\n",
    "        # Generate the answer from the LLM using the constructed prompt\n",
    "        answer = call_claude(prompt, max_tokens, temperature)\n",
    "        answer = answer[0].text\n",
    "        \n",
    "        return answer, best_chunk\n",
    "    else:\n",
    "        print(f\"No relevant chunk found for document {document_title}.\")\n",
    "        return None, None\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for document in data[:1]:\n",
    "    for paragraph in document[\"paragraphs\"][:1]:\n",
    "        for qa in paragraph[\"qas\"][:2]:\n",
    "            question = qa[\"question\"]\n",
    "            document_title = document[\"title\"]  # Pass document title to ensure you query the correct paper\n",
    "\n",
    "            print(f\"Document Title: {document_title}\")\n",
    "\n",
    "            answer, best_chunk = retrieve_answer(question, document_title)\n",
    "            \n",
    "            if answer and best_chunk:\n",
    "                print(f\"Question: {question}\")\n",
    "                print(f\"Context: {best_chunk}\")\n",
    "                print(\"Answer:\", answer)\n",
    "            \n",
    "                # Optionally, store the question and answer (e.g., in a dataframe or list)\n",
    "                dfs.append({\"document_title\": document_title,\"question\": question, \"context\": best_chunk, \"answer\": answer, ground_truth})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in data[:2]:\n",
    "    for paragraph in document[\"paragraphs\"][:2]:\n",
    "        for qa in paragraph[\"qas\"][:2]:\n",
    "            print(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in data[:2]:\n",
    "    for paragraph in document[\"paragraphs\"][:2]:\n",
    "        print(paragraph.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_title = \"CerenceInc_20191002_8-K_EX-10.4_11827494_EX-10.4_Intellectual Property Agreement\"\n",
    "dummy_embedding = [0.0] * 1024  # This should match the dimension of your embeddings\n",
    "\n",
    "# Query ChromaDB, filtering by document title (metadata.doc_id)\n",
    "results = collection.query(\n",
    "    query_embeddings=[dummy_embedding],  # You can use a dummy embedding here\n",
    "    n_results=2,  # Limit to one result for testing\n",
    "    where={\"doc_id\": document_title}\n",
    ")\n",
    "\n",
    "# Print the results for debugging\n",
    "if results['documents']:\n",
    "    print(f\"Found relevant chunk: {results['documents'][0]}\")\n",
    "else:\n",
    "    print(\"No matching chunks found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import chromadb\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and convert to lowercase\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Define a function to chunk text based on token length\n",
    "def chunk_text(text, max_tokens=512):\n",
    "    # Tokenize text\n",
    "    tokens = preprocess_text(text)\n",
    "    chunks = []\n",
    "    \n",
    "    # Ensure we split tokens into chunks of max_tokens size\n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunk = tokens[i:i + max_tokens-5]\n",
    "        \n",
    "        # Reconstruct the chunked text from tokens\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk)\n",
    "\n",
    "        chunked_token_length = len(tokenizer.tokenize(chunk_text))\n",
    "        if chunked_token_length > max_tokens:\n",
    "            print(chunked_token_length)\n",
    "        \n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.Client(Settings())\n",
    "\n",
    "# Create a collection (like a table in a database)\n",
    "collection_name = \"legal_docs\"\n",
    "try:\n",
    "    collection = client.create_collection(name=collection_name)\n",
    "except Exception as e:\n",
    "    collection = client.get_collection(name=collection_name)\n",
    "    print(f\"Collection '{collection_name}' already exists. Using the existing collection.\")\n",
    "\n",
    "MAX_TOKENS = 512 - max_question_length\n",
    "\n",
    "# Populate ChromaDB with document chunks and embeddings\n",
    "for doc_id, document in enumerate(data):\n",
    "    for paragraph in document[\"paragraphs\"]:\n",
    "        context = paragraph[\"context\"]\n",
    "        chunked_text = chunk_text(context, MAX_TOKENS)  # Tokenize and chunk context text\n",
    "        \n",
    "        for i, chunk in enumerate(chunked_text):\n",
    "            # Ensure the chunk ID is unique for each chunk\n",
    "            if i < len(paragraph[\"qas\"]):\n",
    "                qa_id = paragraph[\"qas\"][i][\"id\"]\n",
    "            else:\n",
    "                qa_id = f\"{document['title']}_chunk_{i}\"  # Ensure uniqueness per chunk\n",
    "\n",
    "            # Get embeddings for the chunk\n",
    "            embedding = embedding_model.encode(chunk)\n",
    "\n",
    "            # Add the chunk and its embedding to ChromaDB\n",
    "            print(f\"Adding {qa_id} to collection.\")\n",
    "            collection.add(\n",
    "                ids=[qa_id],  # Use the provided ID for each chunk\n",
    "                documents=[chunk],\n",
    "                metadatas=[{\"doc_id\": document[\"title\"], \"chunk_id\": qa_id}],\n",
    "                embeddings=[embedding]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve answer using ChromaDB\n",
    "def retrieve_answer(question):\n",
    "    # Generate embedding for the question\n",
    "    question_embedding = call_vo_embeddings([question])\n",
    "    \n",
    "    # Query ChromaDB for the most similar chunk\n",
    "    results = collection.query(\n",
    "        query_embeddings=question_embedding.embeddings,\n",
    "        n_results=1  # Return the closest match\n",
    "    )\n",
    "    \n",
    "    # Extract the best chunk text from ChromaDB results\n",
    "    best_chunk = results[\"documents\"][0][0]\n",
    "\n",
    "    print(f\"Current tokens: {count_tokens(best_chunk)}\")\n",
    "    \n",
    "    # Construct the prompt for the LLM, including the question and the most relevant chunk\n",
    "    prompt = f\"Question: {question}\\nContext: {best_chunk}\\nAnswer:\"\n",
    "    \n",
    "    temperature = 0.1\n",
    "    max_tokens = 7000\n",
    "\n",
    "    # Generate the answer from the LLM using the constructed prompt\n",
    "    answer = call_claude(prompt, max_tokens, temperature)\n",
    "    \n",
    "    return answer, best_chunk\n",
    "\n",
    "# List to store answers (optional, for later use)\n",
    "dfs = []\n",
    "\n",
    "# Answer questions from JSON\n",
    "for document in data[:1]:\n",
    "    for paragraph in document[\"paragraphs\"][:1]:\n",
    "        for qa in paragraph[\"qas\"][:1]:\n",
    "            question = qa[\"question\"]\n",
    "            answer, best_chunk = retrieve_answer(question)\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Context: {best_chunk}\")\n",
    "            print(\"Answer:\", answer)\n",
    "            \n",
    "            # Optionally, store the question and answer (e.g., in a dataframe or list)\n",
    "            dfs.append({\"question\": question, \"context\": best_chunk, \"answer\": answer})\n",
    "\n",
    "# df = pd.concat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in data[:1]:\n",
    "    for paragraph in document[\"paragraphs\"][:1]:\n",
    "        for qa in paragraph[\"qas\"][:10]:\n",
    "            print(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in data[:2]:\n",
    "    print( document[\"paragraphs\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc-faq-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
