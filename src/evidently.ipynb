{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evidently for Drift Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import TextEvals\n",
    "from evidently.descriptors import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently.ui.workspace.cloud import CloudWorkspace\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load environment variables from.env file\n",
    "\n",
    "ws = CloudWorkspace(token=os.environ.get(\"EVIDENTLY_API_KEY\"), url=\"https://app.evidently.cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project(id=UUID('019315b8-8594-7683-b69b-e2e0d897358a'), name='FAQ RAG for Law Docs', description='This is a trial project for exploring FAQ RAG and integrating evidently for model drift evaluation.', dashboard=DashboardConfig(name='FAQ RAG for Law Docs', panels=[], tabs=[], tab_id_to_panel_ids={}), team_id=UUID('019315b7-2eda-77d5-9614-ffd7624194b9'), date_from=None, date_to=None, created_at=datetime.datetime(2024, 11, 10, 10, 57, 14, 645204))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = ws.create_project(\"FAQ RAG for Law Docs\", team_id=\"019315b7-2eda-77d5-9614-ffd7624194b9\")\n",
    "project.description = \"This is a trial project for exploring FAQ RAG and integrating evidently for model drift evaluation.\"\n",
    "project.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from base import BASE_DIR\n",
    "\n",
    "df = pd.read_parquet(f\"{BASE_DIR}/output/main/test-001/data.parquet\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"question\", \"answer\"]\n",
    "evaluation_dataset = df[columns]\n",
    "\n",
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"answer\", descriptors=[\n",
    "        Sentiment(),\n",
    "        TextLength(),\n",
    "        IncludesWords(words_list=['sorry', 'apologize'], display_name=\"Denials\"),\n",
    "        ]\n",
    "    ),\n",
    "])\n",
    "\n",
    "text_evals_report.run(reference_data=None, current_data=evaluation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.add_report(project.id, text_evals_report, include_data=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc-faq-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
